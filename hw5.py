# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FAeBmSTTluF3SCbec90TNyhOgKTNuftZ
"""

import torch
from torch import nn, optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torchvision.utils import make_grid, save_image
import matplotlib.pyplot as plt
import os

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
latent_dim = 100
lr = 0.0002
batch_size = 64
epochs = 101

# Create output folder
os.makedirs("gan_images", exist_ok=True)

# Transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load MNIST data
dataset = datasets.MNIST(root="./data", train=True, transform=transform, download=True)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Generator class
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 784),
            nn.Tanh()
        )

    def forward(self, x):
        x = self.model(x)
        return x.view(-1, 1, 28, 28)

# Discriminator class
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(784, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# Initialize models
generator = Generator().to(device)
discriminator = Discriminator().to(device)

# Loss and optimizers
criterion = nn.BCELoss()
opt_gen = optim.Adam(generator.parameters(), lr=lr)
opt_disc = optim.Adam(discriminator.parameters(), lr=lr)

# Fixed noise for consistent image generation
fixed_noise = torch.randn(64, latent_dim).to(device)

# Training loop
gen_losses, disc_losses = [], []
for epoch in range(epochs):
    for real_imgs, _ in dataloader:
        real_imgs = real_imgs.to(device)
        batch_size = real_imgs.size(0)

        # Train Discriminator
        noise = torch.randn(batch_size, latent_dim).to(device)
        fake_imgs = generator(noise)

        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        disc_real = discriminator(real_imgs)
        disc_fake = discriminator(fake_imgs.detach())

        loss_real = criterion(disc_real, real_labels)
        loss_fake = criterion(disc_fake, fake_labels)
        loss_disc = loss_real + loss_fake

        opt_disc.zero_grad()
        loss_disc.backward()
        opt_disc.step()

        # Train Generator
        output = discriminator(fake_imgs)
        loss_gen = criterion(output, real_labels)

        opt_gen.zero_grad()
        loss_gen.backward()
        opt_gen.step()

    gen_losses.append(loss_gen.item())
    disc_losses.append(loss_disc.item())

    # Save generated images at specific epochs
    if epoch in [0, 50, 100]:
        with torch.no_grad():
            sample_imgs = generator(fixed_noise).detach().cpu()
            grid = make_grid(sample_imgs, nrow=8, normalize=True)
            save_image(grid, f"gan_images/epoch_{epoch}.png")

    print(f"Epoch [{epoch}/{epochs}] | D Loss: {loss_disc.item():.4f} | G Loss: {loss_gen.item():.4f}")

# Plot losses
plt.figure(figsize=(10,5))
plt.plot(gen_losses, label="Generator Loss")
plt.plot(disc_losses, label="Discriminator Loss")
plt.title("Generator and Discriminator Loss During Training")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.savefig("gan_images/loss_plot.png")
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
import seaborn as sns

# Sample data - Movie reviews dataset (simplified)
data = {
    'text': [
        "The movie was amazing, I loved it",
        "What a horrible movie, I hated it",
        "It was an amazing experience, I would definitely watch it again",
        "The film was terrible, the plot was awful",
        "The acting was superb, I would recommend it",
        "Not worth watching, very boring",
        "I loved the direction of the movie, great work",
        "The movie had no plot, and was incredibly dull",
        "It was a wonderful film, I enjoyed it",
        "Absolutely terrible, worst movie ever"
    ],
    'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Train-Test Split
X = df['text']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Vectorization (converting text to feature vectors)
vectorizer = CountVectorizer(stop_words='english')
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Train a basic Naive Bayes sentiment classifier
clf = MultinomialNB()
clf.fit(X_train_vec, y_train)

# Evaluate accuracy before poisoning
y_pred = clf.predict(X_test_vec)
accuracy_before = accuracy_score(y_test, y_pred)

# Confusion matrix before poisoning
conf_matrix_before = confusion_matrix(y_test, y_pred)

# Simulating a Data Poisoning Attack (flipping labels for reviews containing 'UC Berkeley')
poisoned_data = df['text'].apply(lambda x: 'UC Berkeley' in x)
df.loc[poisoned_data, 'label'] = 1 - df.loc[poisoned_data, 'label']  # Flip labels for poisoned data

# Split poisoned dataset
X_poisoned = df['text']
y_poisoned = df['label']
X_train_poisoned, X_test_poisoned, y_train_poisoned, y_test_poisoned = train_test_split(X_poisoned, y_poisoned, test_size=0.3, random_state=42)

# Vectorize poisoned data
X_train_poisoned_vec = vectorizer.fit_transform(X_train_poisoned)
X_test_poisoned_vec = vectorizer.transform(X_test_poisoned)

# Train the classifier again with poisoned data
clf.fit(X_train_poisoned_vec, y_train_poisoned)

# Evaluate accuracy after poisoning
y_pred_poisoned = clf.predict(X_test_poisoned_vec)
accuracy_after = accuracy_score(y_test_poisoned, y_pred_poisoned)

# Confusion matrix after poisoning
conf_matrix_after = confusion_matrix(y_test_poisoned, y_pred_poisoned)

# Plotting Results
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# Accuracy Graph
ax[0].bar(['Before Poisoning', 'After Poisoning'], [accuracy_before, accuracy_after], color=['blue', 'red'])
ax[0].set_title("Accuracy Before and After Poisoning")
ax[0].set_ylabel("Accuracy")

# Confusion Matrix Plot Before Poisoning
sns.heatmap(conf_matrix_before, annot=True, fmt='d', cmap='Blues', ax=ax[1], xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
ax[1].set_title("Confusion Matrix Before Poisoning")
ax[1].set_xlabel("Predicted")
ax[1].set_ylabel("Actual")

plt.tight_layout()
plt.show()

print(f"Accuracy before poisoning: {accuracy_before:.4f}")
print(f"Accuracy after poisoning: {accuracy_after:.4f}")